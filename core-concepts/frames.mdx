---
title: Frames
description: 'Understanding frames - the fundamental unit of data flowing through Pipecat pipelines.'
---

## What are Frames?

Frames are the fundamental unit of data in Pipecat. Like packages on a conveyor belt, each frame contains specific data and flows through your application's pipeline. Every piece of information - from audio samples to LLM responses - travels through your application as a frame.

## Frame Categories

### DataFrame

Base class for all data-carrying frames. These frames contain the actual content flowing through your application:

```python
# Audio data (e.g., from microphone)
class AudioRawFrame(DataFrame):
    audio: bytes
    sample_rate: int
    num_channels: int

# Text content
class TextFrame(DataFrame):
    text: str

# Images
class ImageRawFrame(DataFrame):
    image: bytes
    size: Tuple[int, int]
    format: str | None

# LLM conversation context
class LLMMessagesFrame(DataFrame):
    messages: List[dict]
```

### SystemFrame

Frames for system control that can bypass normal processing order:

```python
# Common system frames
ErrorFrame("Network error")       # Signal error conditions
StartInterruptionFrame()          # User started speaking
StopInterruptionFrame()           # User stopped speaking
UserStartedSpeakingFrame()        # Speech detection
UserStoppedSpeakingFrame()        # Speech end detection
```

### ControlFrame

Frames for pipeline flow control:

```python
# Common control frames
StartFrame()                  # Initialize pipeline
EndFrame()                    # Normal termination
LLMFullResponseStartFrame()   # Start of LLM response
LLMFullResponseEndFrame()     # End of LLM response
TTSStartedFrame()             # Start speech synthesis
TTSStoppedFrame()             # End speech synthesis
```

### AppFrame

Base class for your custom application-specific frames:

```python
@dataclass
class CustomStateFrame(AppFrame):
    state: dict
    timestamp: str
```

## Frame Relationships

Frames work together to create complete interactions. Here's how frames flow in a typical voice interaction:

```python
# Complete voice interaction example
UserStartedSpeakingFrame()           # User begins speaking
InputAudioRawFrame(audio_data)       # Raw audio input
TranscriptionFrame("Hello")          # Speech converted to text
OpenAILLMContextFrame(messages)      # Context for LLM
TextFrame("Hi, how can I help?")     # LLM response
TTSAudioRawFrame(audio_response)     # Synthesized speech
UserStoppedSpeakingFrame()           # User finished speaking
```

## Common Patterns

Frames work together in common patterns to manage interactions and state changes in your application. Here are some key patterns you'll use frequently:

### Turn Taking

In conversational applications, frames help manage who is speaking and when. This ensures natural dialogue flow and prevents talking over each other:

```python
# Managing turns in conversation
UserStartedSpeakingFrame()    # User's turn begins
StartInterruptionFrame()      # Interrupt if bot is speaking
UserStoppedSpeakingFrame()    # User's turn ends
BotStartedSpeakingFrame()     # Bot's turn begins
```

### State Changes

Your application needs to know when important states change, like when speech synthesis starts or stops, or when errors occur. Frames provide a clean way to signal these changes:

```python
TTSStartedFrame()            # Bot begins speaking
TTSStoppedFrame()            # Bot finished speaking
ErrorFrame("Network error")  # Something went wrong
```

## Frame Flow

Frames can move in two directions through your pipeline, enabling both normal processing and control signals. The direction of flow determines how frames are handled:

```python
class FrameDirection(Enum):
    DOWNSTREAM = 1  # Normal processing flow
    UPSTREAM = 2    # Error reporting and control signals
```

A typical pipeline shows how different frame types flow through your application:

```python
pipeline = Pipeline([
    transport.input(),   # Creates InputAudioRawFrame
    transcriber,         # Produces TranscriptionFrame
    llm_processor,       # Produces TextFrame
    tts_service,         # Produces TTSAudioRawFrame
    transport.output()   # Handles audio output
])
```

Each processor in the pipeline follows these rules:

1. Receives frames
2. Processes relevant frames by type
3. Generates new frames as needed
4. Passes through frames it doesn't handle

## Design Principles

### Immutability

Frames should be treated as immutable. Instead of modifying frames, create new ones:

```python
# Good: Create new frames for transformed data
text_frame = TranscriptionFrame(transcribed_text)
await self.push_frame(text_frame)

# Bad: Don't modify existing frames
frame.text = new_text  # Avoid this!
```

### Single Responsibility

Each frame type should have a clear purpose:

```python
# Specific frame types for specific purposes
UserStartedSpeakingFrame()  # Speech detection only
TranscriptionFrame(text)    # Speech content only
ErrorFrame(error)           # Error conditions only
```

### Frame Ordering

Frames maintain their order through the pipeline, with two exceptions:

1. System frames bypass normal ordering
2. Upstream frames (like errors) flow backward

```python
async def process_frame(self, frame: Frame, direction: FrameDirection):
    if isinstance(frame, SystemFrame):
        # 1. System frames bypass normal ordering
        await self.handle_system_frame(frame)
    elif direction == FrameDirection.UPSTREAM:
        # 2. Upstream frames (like errors) flow backward
        await self.handle_upstream(frame)
    else:
        # Normal frames maintain order
        await self.process_ordered_frame(frame)
```

## Using Frames Effectively

Understanding how to handle frames in your processors and pipelines is key to building effective applications. Here are common patterns for working with frames:

### In Processors

Processors should handle different frame types appropriately, paying special attention to system events and direction:

```python
async def process_frame(self, frame: Frame, direction: FrameDirection):
    # Handle system events immediately
    if isinstance(frame, StartInterruptionFrame):
        await self.stop_processing()
        return

    # Process data frames normally
    if isinstance(frame, AudioRawFrame):
        text = await self.transcribe(frame.audio)
        await self.push_frame(TranscriptionFrame(text))

    # Pass through other frames
    await self.push_frame(frame, direction)
```

### In Pipelines

Pipelines coordinate complex operations by connecting processors that handle different frame types. Each processor focuses on its specific task while maintaining the overall flow:

```python
pipeline = Pipeline([
    audio_input,         # Produces AudioRawFrame
    vad_processor,       # Produces speech detection frames
    transcriber,         # Produces TranscriptionFrame
    context_manager,     # Manages conversation context
    llm_service,         # Generates responses
    tts_service          # Converts to speech
])
```

## Next Steps

Now that you understand frames, learn how they're processed in:

- [Frame Processors](/core-concepts/processors)
- [Pipelines](/core-concepts/pipelines)
