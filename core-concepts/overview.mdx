---
title: Core Concepts Overview
description: 'Understanding how Pipecat works: frames, processors, pipelines, and transport.'
---

Pipecat uses a pipeline-based architecture to handle real-time AI processing. Let's look at how this works in practice, then break down the key components.

## Real-time Processing in Action

Consider how a voice assistant processes a user's question and generates a response:

<Frame>![Real-time processing pipeline](/images/architecture-1.svg)</Frame>

Instead of waiting for complete responses at each step, Pipecat processes data in small units called frames that flow through the pipeline:

1. Speech is transcribed in real-time as the user speaks
2. Transcription is sent to the LLM as it becomes available
3. LLM responses are processed as they stream in
4. Text-to-speech begins generating audio for early sentences while later ones are still being generated
5. Audio playback starts as soon as the first sentence is ready

This streaming approach creates natural, responsive interactions.

## Architecture Overview

Here's how Pipecat organizes these processes:

<Frame>![Platform architecture](/images/architecture-2.svg)</Frame>

The architecture consists of four key components:

### 1. Frames

Frames are containers for data moving through your application. Think of them like packages on a conveyor belt - each contains a specific type of cargo:

- Audio data from a microphone
- Text from transcription
- LLM responses
- Generated speech audio
- Images or video
- Control signals and system messages

### 2. Processors (Services)

Processors are workers along our conveyor belt. Each one:

- Receives frames
- Processes specific frame types
- Passes through frames it doesn't handle
- Generates new frames as output

For example:

- A text-to-speech processor only handles text frames, converting them to audio frames
- An LLM processor takes context frames and produces text frames
- A logging processor might watch all frames but not modify them

### 3. Pipelines

Pipelines connect processors together, creating a path for frames to flow through your application. They can be:

```python
# Simple linear pipeline
pipeline = Pipeline([
    transcriber,          # Speech -> Text
    llm_processor,        # Text -> Response
    tts_service,         # Response -> Audio
])

# Complex parallel pipeline
pipeline = Pipeline([
    input_source,
    ParallelPipeline([
        [image_processor, image_output],    # Handle images
        [audio_processor, audio_output]     # Handle audio
    ])
])
```

### 4. Transport

Transport is your application's gateway to the real world. It:

- Receives input (audio, video, etc.)
- Converts input into frames
- Takes output frames
- Renders them for users (playing audio, showing images)

## How It All Works Together

Let's see how these components handle a simple voice interaction:

1. **Input**

   - User speaks into their microphone
   - Transport converts audio into frames
   - Frames enter the pipeline

2. **Processing**

   - Transcription processor converts speech to text frames
   - LLM processor takes text frames, generates response frames
   - TTS processor converts response frames to audio frames

3. **Output**
   - Audio frames reach the transport
   - Transport plays the audio for the user

This happens continuously and in parallel, creating smooth, real-time interactions.

## Next Steps

Now that you understand the big picture, let's dive deeper into each component:

- [Frames](/core-concepts/frames) - Learn about different frame types and their properties
- [Processors](/core-concepts/processors) - Understand how to process and transform frames
- [Pipelines](/core-concepts/pipelines) - Master pipeline construction and patterns

Need help? Join our [Discord community](https://discord.gg/pipecat) for support and discussions.
